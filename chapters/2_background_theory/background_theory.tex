%
% File: chap01.tex
% Author: Oliver J. H. Feighan
% Description: Introduction chapter
%
\let\textcircled=\pgftextcircled
\chapter{Background Theory}
\label{chap:background_theory}

\initial {T}here are three main components to calculating atomic geometry dependent
Frenkel-Davydov exciton models. First is the underlying electronic structure theory, 
such as density functional theory (DFT) or density functional tight binding (DFTB).
The electronic structure theory is the basis for calculating excited states, providing
MO coefficients for electron integrals, orbital energies and other properties. The 
second component is the excited state method, again such as time-dependent DFT (TD-DFT)
or TD-DFTB. In this work other excited state methods are discussed so these theories 
are outlined in this chapter as well. The excited state methods provide the transition 
properties, such as transition densities, charges or dipoles and transition energies 
that are used to construct the exciton Hamiltonian. The third component is the exciton 
Hamiltonian itself, which depends on which energies to include and how to calculate 
the coupling parameters. In order to establish a basis for reporting and discussion 
in later chapters, outlines of each of these three components are given here.

\section{Electronic structure}
\label{sec:electronic_structure}

\subsection{Density Functional Theory}
\label{subsec:dft}

Density functional theory is ubiquitous in molecular chemistry simulations \cite{Maitra2016}.
Often this theory is chosen for its good scaling with respect to system sizes as 
well as an appreciable accuracy against higher level methods. A brief overview of
the DFT formalism is given here.

At its heart DFT is based on the two Hohenberg-Kohn theorems \cite{Kohn1964}. The
first states that the ground state energy $E_{\text{GS}}$, is proven to have a one-to-one
mapping to a functional of the electron density $\rho_{\text{GS}} \left(r\right)$

\begin{equation}
    E_{\text{GS}} : \rho_{\text{GS}} \left(r\right) \mapsto E \left[ \rho_{\text{GS}} \left(r\right)\right],
\end{equation}
%
where $E \left[ \rho_{\text{GS}} \left(r\right)\right]$ is the functional. The second
theorem is closely related to the variation principle, stating that the \emph{exact}
ground state density also minimizes the total energy. This minimum corresponds to 
only one electron density. Whilst proven in principle, the exact functional of the
electron density which gives this energy is unknown, and so various approximations
have be made. One popular approximation is the Kohn-Sham approach, where electrons
are approximated as non-interacting in order to tractably calculate the ground state
density \cite{Kohn1965}. The total energy in the Kohn-Sham approach is the sum of 
functionals

\begin{equation}
    \functional{E}{\text{tot}}{\rho\left(r\right)} = \functional{E}{T_S}{\rho\left(r\right)} + \functional{E}{V}{\rho\left(r\right)} + \functional{E}{J}{\rho\left(r\right)} + \functional{E}{X}{\rho\left(r\right)} + \functional{E}{C}{\rho\left(r\right)},
\end{equation}
%
where the terms $\functional{E}{T_S}{\rho\left(r\right)}$, $\functional{E}{V}{\rho\left(r\right)}$,
$\functional{E}{J}{\rho\left(r\right)}$, $\functional{E}{X}{\rho\left(r\right)}$, 
$\functional{E}{C}{\rho\left(r\right)}$ correspond to the kinetic, (nuclear) potential, 
bare Coulombic, exchange and correlation interaction respectively. It is necessary 
to include the exchange and correlation terms as the Coulombic approximation does 
not include any treatment of spin effects. The solution for the minimum energy would
satisfy the equation

\begin{equation}
    \Delta \left[  \functional{E}{\text{tot}}{\rho\left(r\right)}  - \mu \left( \int_{}^{} \rho\left(r\right) \,dr - N \right) \right] = 0
\end{equation}
%
with the constraint that the total number of electrons $N$ is conserved. The value
of $\mu$, the Lagrange multiplier, is given by

\begin{equation}
    \mu = \frac{\delta  \functional{E}{\text{tot}}{\rho\left(r\right)}}{\delta \rho\left(r\right)},
\end{equation}
%
which can be rewritten in terms of the kinetic energy and energy potentials as

\begin{equation}
    \begin{split}
        \mu &= \frac{\delta  \functional{E}{T_S}{\rho}}{\delta \rho\left(r\right)} + \functional{v}{V}{\rho\left(r\right)} + \functional{v}{J}{\rho\left(r\right)} + \functional{v}{X}{\rho\left(r\right)} + \functional{v}{C}{\rho\left(r\right)} \\
            &= \frac{\delta  \functional{E}{T_S}{\rho}}{\delta \rho\left(r\right)} + \functional{v}{KS}{\rho\left(r\right)},
    \end{split}
\end{equation}
%
where the energy potentials are combined into the Kohn-Sham potential $\functional{v}{KS}{\rho\left(r\right)}$ for
convenience. As the electrons are non-interacting, this potential can be used to
solve one-electron Schr√∂dinger equations

\begin{equation}
    \left[ -\frac{1}{2} \nabla^2 + \functional{v}{KS}{\rho\left(r\right)} \right] \psi_i = \epsilon_i \psi_i
\end{equation}
%
where $\psi_i$ are one-electron wavefunctions, $\epsilon_i$ are Lagrange multipliers
to ensure orthonormality (also interpreted as orbital energies), and the kinetic 
energy term $-\frac{1}{2} \nabla ^2$ is given from the definition of the kinetic 
energy of non-interacting electrons as

\begin{equation}
    \functional{E}{T_S}{\rho\left(r\right)} = -\frac{1}{2} \sum_i^N \braket{\psi_i|\nabla^2|\psi_i}.
\end{equation}
%
The total electron density can then be constructed from the one-electron wavefunction
solutions

\begin{equation}
    \rho = \sum_i^N \left\lvert \psi_i \right( r \left)^2 \right\rvert.
\end{equation}
%
Again the issue is that the potential functional $\functional{v}{KS}{\rho\left(r\right)}$ is
not known and so approximations have to be made. Additionally an initial guess of
the electron density is needed to start the variational procedure, but this can 
readily be given from atomic densities or other methods \cite{Lehtola2019}.

In practice a range of exchange-correlation functionals are employed dependent on
the problem at hand. The simplest functionals employ the local density approximation
(LDA), which assumes the electron density is the same as the uniform electron gas
for all points. More complicated functionals use the electron density and electron
density gradient, using the generalized gradient approximation (GGA) \cite{Langreth1983}.
Other examples include meta-GGAs, which go further and use the second derivative, 
as well as hybrid functionals, which use some fraction of the Hartree-Fock energy
\cite{Sun2011, Becke1993}. Many exchange-correlation functionals require parameterization 
against high level data.

The first DFT calculations were done on periodic systems with infinite-domain plane
wave functions used as a basis set. However molecular electronic structure packages
often use basis functions centred on atomic positions \cite{Boys1950}, although 
this is not always the case. The one-electron wavefunctions can be written as linear
combinations of basis functions $\chi_j$

\begin{equation}
    \psi_i = \sum_\mu^n c_{\mu i} \chi_\mu \left( r\right),
\end{equation}
%
where $\mu$ is the index of the basis function, going up to the total number of functions
$n$, and $c_{\mu i}$ is the molecular orbital (MO) coefficient of the basis function
$\chi_\mu$ (also referred to as an atomic orbital or AO) for orbital (one-electron 
wavefunction) $\psi_i$. Rewriting the electron density gives the density matrix 
$\mathbf{P}$

\begin{equation}
    \rho = \sum_{\mu\nu} P_{\mu\nu} \chi_\mu \chi_\nu,
\end{equation}
%
where the elements of the density matrix are the product of MO coefficients

\begin{equation}
    P_{\mu\nu} = \sum_i^N c_{i \mu} c_{i \nu}.
\end{equation}
%
Similarly, the Kohn-Sham potential can be written in matrix form as

\begin{equation}
    F_{\mu \nu} = \frac{\delta E_{KS}}{\delta P_{\mu\nu}},
\end{equation}
%
where $E_{KS}$ is the energy from the potential $v_{KS}$. Applying the variational
principle gives the matrix equation

\begin{equation}
    \mathbf{F} \mathbf{C} = \mathbf{S} \mathbf{C} \mathbf{\epsilon},
\end{equation}
%
where $\mathbf{S}$ is the overlap matrix and $\mathbf{\epsilon}$ is the diagonal 
matrix of one-electron wavefunction energies. This equation is similar to the
Roothaan-Hall equations used to solve Hartree-Fock theory, and so the matrix $\mathbf{F}$
is commonly referred to as the Fock matrix \cite{Roothaan1951}. As the Fock matrix
is dependent on the MO coefficient $\mathbf{C}$, solving this matrix equation requires
an iterative procedure to converge to a set of MO coefficients that can be calculated
self-consistently. This procedure is referred to as a self-consistent field or SCF.

Having a convenient definition of the ground state electronic structure is useful
in calculating other properties. As explained in section \ref{subsec:tddft}, the
ground state MO coefficients can be used to calculate vertical excitation energies
as well as excited state and transition electron densities. These properties are
necessary to understand the photochemical processes in light harvesting complexes.

\subsection{Density Functional Tight Binding}
\label{subsec:tight_binding}
In recent years there has been renewed interest in tight-binding (TB) methods, including
tight binding methods derived from density functional theory (named density function 
tight binding or DFTB) \cite{Porezag1994}. Interest in DFTB and similar methods 
has grown in the last $\sim$10 years as they are often more accurate than force-field
methods, especially for complex metallic systems, and cheaper than DFT methods. 
Modern applications combine semi-empirical tight-binding methods with machine-learning
to accelerate screening of potential drug candidates \cite{Qiao2020}, materials 
\cite{Hegde2017} and organic electronics \cite{Omar2021}. These methods approximate 
the density functional energy by expanding into a Taylor series, based on the density
fluctuations $\delta\rho$

\begin{equation}
    \begin{split}
    E\left[\rho\right] &= E^{\left(0\right)}\left[\rho_0\right] + E^{\left(1\right)}\left[\rho_0, \delta\rho\right] + E^{\left(2\right)}\left[\rho_0, \left(\delta\rho\right)^2\right] + E^{\left(3\right)}\left[\rho_0, \left(\delta\rho\right)^3\right] + \dots \\
    &= \functional{E}{}{\rho_0\left(r\right)} \\
    &+ \left. \int \frac{\delta \functional{E}{}{\rho\left(r\right)}}{\delta \rho\left(r\right)} \right\rvert_{\rho_0}  \delta \rho \left( r \right) \\
    &+ \frac{1}{2} \left. \int\int \frac{\delta^2 \functional{E}{}{\rho\left(r\right)}}{\delta \rho\left(r\right)\delta \rho\left(r^\prime\right)} \right\rvert_{\rho_0} \delta \rho \left( r \right) \delta \rho \left( r^\prime \right) \\
    &+ \frac{1}{p!} \left. \int\int \cdots\int \frac{\delta^p \functional{E}{}{\rho\left(r\right)}}{\delta \rho\left(r\right)\delta \rho\left(r^\prime\right) \cdots \delta \rho\left(r^{\left(p\right)}\right)} \right\rvert_{\rho_0} \delta \rho \left( r \right) \delta \rho \left( r^\prime \right) \cdots \delta \rho\left(r^{\left(p\right)}\right) \\
    &+ \cdots
    \end{split}
\end{equation}
%
with this series usually truncated between the first and third term\cite{Gaus2011, Koskinen2009}. 
These terms are analogous to some of the terms in the Kohn-Sham functional. The 
first term, which does not contain density fluctuation terms, is called the band-structure
energy and is given by the sum of energies of single particle wavefunction

\begin{equation}
    \functional{E}{}{\rho_0\left(r\right)} =  \sum_i \braket{\psi_i|\functional{H}{}{\rho_0}|\psi_i},
\end{equation}
%
where the Hamiltonian $H$ contains the kinetic energy and electron-nuclear potential
(often called the core Hamiltonian). The second order term corresponds to the Coulomb 
and exchange-correlation terms

\begin{equation}
    \frac{1}{2} \left. \int\int \frac{\delta^2 \functional{E}{}{\rho\left(r\right)}}{\delta \rho\left(r\right)\delta \rho\left(r^\prime\right)} \right\rvert {\rho_0} \delta \rho \left( r \right) \delta \rho \left( r^\prime \right) = \frac{1}{2} \left. \int\int \frac{\delta^2 \functional{E}{XC}{\rho\left(r\right)}}{\delta \rho\left(r\right)\delta \rho\left(r^\prime\right)} + \frac{1}{\left\lvert r-r^\prime \right\rvert} \right\rvert_{\rho_0} \delta \rho \left( r \right) \delta \rho \left( r^\prime \right),
\end{equation}
%
with the other Taylor expansion terms collected into what is known as the repulsive
energy term $E^\text{rep}$. The common expression for DFTB (also known as DFTB2
or SCC-DFTB \cite{Elstner1998}) energy is then given as the sum

\begin{equation}
    \label{eq:dftb_energies}
    E^{\text{DFTB}} = \sum_i \braket{\psi_i|\functional{H}{}{\rho_0}|\psi_i} + E^\text{XC} + E^\text{rep}.
\end{equation}
%
However it is common to include other correctional terms in this framework, as discussed
in section \ref{subsec:xtb_methods}. It is also common to replace the continuous 
electron density with a point charge model. Calculating energy terms with a point
charge model instead of electron integrals is key in accelerating TB methods. Charges
are determined by first approximating the charge fluctuations as a sum of fluctuations 
centred on atomic positions

\begin{equation}
    \delta \rho\left(r\right) = \sum_A \delta \rho_A \left(r\right),
\end{equation}
%
where $\rho_A \left(r\right)$ is the charge fluctuation on atom $A$. These atomic
contributions are then expanded using a multipole expansion, truncated at the first
term

\begin{equation}
    \delta \rho_A \left(r\right) \approx \Delta q_A F_A^{00} \Upsilon^{00},
\end{equation}
%
where $F_A^{00}$ and $\Upsilon^{00}$ are the multipole expansion coefficients - 
$\Delta q_A$ is commonly referred to as the partial charge on atom $A$. As these
charges are not an observable of the system but an approximation of the electronic
density it is arbitrary which charge scheme is used. Often these are Mulliken charges 
but other methods can be used as can be seen in section \ref{subsec:stda_xtb}. The
benefit of using a point charge approximation to electron density is that integrals
are far less expensive to calculate. The exchange-correlation term is then given
by

\begin{equation}
    E^{\text{XC}} = \frac{1}{2}\sum_{A,B} \Delta q_A \Delta q_B \gamma_{AB},
    \label{eq:dftb_e_ex}
\end{equation}
%
where the function $\gamma_{AB}$ recovers the properties of electron-electron interactions.
For example at large separation this function tends towards the Coulombic $\frac{1}{R_{AB}}$
interaction. At close separations, the $\gamma$ function uses the chemical hardness 
of atoms to dampen this interaction. For self-interaction (i.e. $\gamma_{AA}$), 
the energy term is equivalent to the second derivative of the energy with respect
to the atomic partial charge, equal to the Hubbard parameter $U_{A}$ or twice the chemical
hardness $\eta_{A}$

\begin{equation}
    \begin{split}
    E^{\text{XC}}_{AA} &= \frac{1}{2} \Delta q_A^2 \gamma_{AA} \\
    &= U_{A} \\
    &= 2 \eta_A.
    \end{split}
\end{equation}
%
For inter-atomic interactions the $\gamma$ function is scaled by the Hubbard parameters
of both atoms involved.

It is also assumed that density fluctuations will only occur in the valence space
of atoms, and so core atomic orbitals do not need explicit treatment. As such, DFTB
methods usually use a minimal valence basis set \cite{Bannwarth2020}. These valence
atomic orbitals require orthogonalisation against the core orbitals of other atoms,
usually achieved with a Schmit orthogonalisation. They are also often calculated
from atomic DFT data, with an additional harmonic constraint to prevent
the orbitals from becoming too diffuse. Often this confinement potential is scaled 
to be within twice the covalent radius of each atom.

Solving for the ground state wavefunction employs a similar Roothaan-Hall matrix
equation, although with the Fock matrix defined by equation \ref{eq:dftb_energies}.
Whilst in normal DFT the convergence of the potential (or Fock matrix) between iterations
indicates that the ground state solution has been found, many DFTB schemes use the
difference in atomic charges. This scheme is called self-consistent charges (SCC),
and is common in many tight binding models.

Whilst some parameters can be calculated from DFT or \emph{ab initio} calculations,
such as the Hubbard parameters and AO coefficients, many parameters require fitting
against higher level or empirical data. For example the parameters for the repulsive
energy term are often fit to bond length data. Additionally, many elements can be
precomputed. This again is key to obtain higher efficiency than DFT methods. One
common example is the the core energy functional, which be written in terms of the 
AO contributions as

\begin{equation}
    \functional{E}{}{\rho_0\left(r\right)} = \sum_i \sum_{\mu\nu} c_{i\mu}c_{i\nu} H_{\mu\nu}
\end{equation}
%
where the elements of $H_{\mu\nu}$, the Hamiltonian in the atomic basis, can be 
precomputed as they are not dependent on the density fluctuations $\delta \rho$. 
Similarly elements of the AO overlap matrix $S_{\mu\nu}$ can be precomputed and 
tabulated. For many parameters a careful approach has to be taken to avoid systematic 
accuracy issues.

Much work has been done on benchmarking DFTB and expanding the formalism \cite{Koskinen2009, Goldman2012, AndreaRozzi2004, Kohler2005, Niehaus2005, Han2000}. 
Generally the accuracy of DFTB is found to be on par with or just below DFT \cite{Lutsker2015, Gruden2017, Vuong2018},
and higher order derivative terms can be used to achieve more accurate results. 
Often it is found that the repulsive energy term is the trickiest to get correct, 
requiring the most reference data for parameterisation \cite{Koskinen2009}. Additional 
energy terms, such as non-covalent interactions and spin-polarisation effects can 
also be included. The spin-polarisation is especially useful for excited states \cite{Melix2016}. 
Originally DFTB was formulated in a restricted ansatz, using doubly occupied orbitals 
instead of spin-orbitals. This approach restricted its application in a linear response
framework as the excited states could not be properly treated. However additional 
energy terms in the Fock matrix recover the effects of using a spin-unrestricted 
ansatz, making predictions of excited states much more accurate.

Tight binding methods are usually best used in investigations where the scale of 
the  system of interest is too large for more usual methods, such as DFT or Hartree-Fock 
(HF) based methods. Alternative solutions for dealing with the size of these systems 
include force-field methods, which do not use any quantum mechanics theory and only
use classical methods to evaluate energies and gradients of systems. However, it
has routinely been shown that these are inaccurate for many systems that involve 
proton transfer, metallic centers or the making and breaking of chemical bonds\cite{Salomon-Ferrer2013},
which covers many interesting biochemical systems including LHCs. For these systems
using tight-binding methods seems to be a good trade-off between the expense of 
full DFT methods and the inaccuracies of classical methods. However, work on making 
DFT programs quicker, usually with efficient massively parallelized codes, is closing 
this gap where DFTB methods are useful \cite{Manathunga2020}.

\subsection{Extended Tight Binding}
\label{subsec:xtb_methods}

Recently the extended tight binding (xTB) family of methods, developed by Grimme 
and coworkers, have been presented as another semi-empirical tight binding solution
to investigating large chemical systems \cite{Bannwarth2020, Bannwarth2019, Grimme2017, Pracht2019, Grimme2016, Spicher2020a}.
Many of these methods have been parameterized for geometry optimizations and frequencies 
of normal modes and use novel approaches for non-covalent interactions. These are
identified by the GFN prefix (Geometries, Frequencies and Non-covalent). These methods
require far fewer pair-wise parameters, intending to emulate the ZDO type methods,
whilst remaining efficient and accurate for large systems. The issue with pair-wise 
parameters is similar to over-fitting in semi-empirical models. For example if one 
component uses element wise parameters for atoms up to radon (the original cutoff 
of GFN1-xTB) then 86 parameters are required - if using pair-wise parameters then 
7,396 parameters are needed. This will require more training data as the number
of training features should be much greater than the number of parameters needed 
to be fit, making datasets more expensive to calculate. Fitting more parameters 
is also generally more difficult to optimize for many heuristics. Other issues with
these highly specific parameters is that they are rarely transferable to other chemical
environments and often require reparameterisation for different target properties. 
They are often limited to the upper parts of the periodic table for this reason. 
Minimizing pair-wise parameters is then beneficial in terms of training set size, 
performance and re-usability. The number of parameters required varies systematically 
across the family of GFN methods, decreasing with increasingly sophisticated descriptions
of the electronic structure. For example GFN2 uses a quadrupole expansion for electrostatics, 
which, along with a detailed dispersion method, makes GFN2 completely pairwise parameter 
free. GFN0 and GFN-FF on the other hand use very approximate methods and more parameters 
to increase computational efficiency. 

The energy terms for xTB methods can be characterized by the order of density fluctuations 
they correspond to. For example, the zeroth order terms correspond to dispersion 
(either D3 \cite{Grimme2010}, D4 \cite{Caldeweyher2020} or a modified D4 method) and
a halogen bonding correction. First order terms are calculated with an extended 
H\"{u}ckel theory, and second and higher order terms are calculated by isotropic
electrostatic and exchange-correlation terms. In the following equations, these energies
are first labelled with a superscript $\left(n\right)$ to identify the density fluctuation 
order, and a subscript to describe the interaction calculated. The second line in
each equation gives the corresponding label found in the GFN-xTB publications. The
GFN-xTB energies are then summarized as

\newcommand{\orderE}[2]{E^{\left(#1\right)}_{#2}}
\newcommand{\nameE}[2]{E^{#1}_{#2}}
\begin{equation}
\begin{aligned}
E_{\text{GFN1-xTB}} &= \orderE{0}{\text{disp}} + \orderE{0}{\text{rep}} + \orderE{0}{\text{XB}} + \orderE{1}{\text{EHT}} + \orderE{2}{\text{IES+IXC}} + \orderE{3}{\text{IES+IXC}} \\
&= \nameE{\text{D3}}{\text{disp}} + \nameE{}{\text{rep}} + \nameE{\text{GFN1}}{\text{XB}} + \nameE{}{\text{EHT}} + \nameE{}{\gamma} + \nameE{\text{GFN1}}{\Gamma}
\end{aligned}
\end{equation}
%
\begin{equation}
\begin{aligned}
E_{\text{GFN2-xTB}} &= \orderE{0,1,2}{\text{disp}} + \orderE{0}{\text{rep}} + \orderE{1}{\text{EHT}}  + \orderE{2}{\text{IES+IXC}} + \orderE{2}{\text{AES+AXC}} + \orderE{3}{\text{IES+IXC}} \\
&= \nameE{\text{D4'}}{\text{disp}} + \nameE{}{\text{rep}} + \nameE{}{\text{EHT}}  + \nameE{}{\gamma} + \nameE{}{\text{AEC}} + \nameE{}{\text{AXC}} + \nameE{\text{GFN2}}{\Gamma}
\end{aligned}
\end{equation}
%
\begin{equation}
\begin{aligned}
E_{\text{GFN0-xTB}} &= \orderE{0}{\text{disp}} + \orderE{0}{\text{rep}} + \orderE{1}{\text{EHT}}  + \Delta\orderE{0}{} \\
&= \nameE{\text{D4}}{\text{disp}} + \nameE{}{\text{rep}} + \nameE{}{\text{EHT}}  + \nameE{}{\text{EEQ}} + \nameE{}{\text{srb}}
\end{aligned}
\end{equation}
%
Common to all of these expressions is the extended H\"{u}ckel theory energy term ($\nameE{}{\text{EHT}}$),
derived from first order density fluctuations. This energy is given by tracing the
Hamiltonian

\begin{equation}
    H_{\mu\nu}^{\text{EHT}} = \frac{1}{2} K_{AB}^{ll'}S_{\mu\nu}\left(H_{\mu\mu} + H_{\nu\nu}\right)X\left(EN_A, EN_B\right)\Pi\left(R_{AB}, l, l'\right)Y\left(\eta^A_l\eta^B_{l'}\right)
\end{equation}
%
with the valence one-electron density $P_{\mu\nu}$

\begin{equation}
E_{\text{EHT}} = \sum_{\mu\nu} P_{\mu\nu} H_{\mu\nu}^{\text{EHT}},
\end{equation}
%
where $\mu$, $\nu$ are the indices of atomic orbitals. In the equation above $A,B$
are atomic indices, and $l,l'$ are the indices of atomic orbitals on atoms $A, B$
respectively (ie $l \in A, l' \in B$), $K_{AB}^{ll'}$ are parameterized global scaling
terms, $S$ is the atomic orbital overlap, $H_{\mu\mu}$ are diagonal elements of 
the Hamiltonian, treating on-site energies, $X$ is an environment-scaled electronegativity
$EN$ function, $\Pi$ is another distance-dependent function to correct for the distance-scaled
interactions from the overlap matrix and $Y$ corrects for kinetic energy integrals
in GFN2- and GFN0-xTB but is discarded for GFN1-xTB.

Due to the lack of element pair-wise parameters (except for a few special cases 
in the global scaling constants), each interaction term is readily separable and
so can either be considered or discarded for future parameterization work. For example, 
the $\Pi$, $Y$ and $X$ terms deal with interactions that are more strongly influenced
by atomic environments rather than particular chemistry between atoms. Hence, for 
excited state theories they would not really have to be changed. This is corroborated 
by their absence in the Hamiltonian for the simplified Tamm-Dancoff (sTDA) xTB \cite{Grimme2016} 
method, which is discussed below.

In the GFN1 and GFN2 methods there are also the $E_\gamma$ and $E_\Gamma^{\text{GFNx}}$ 
terms, which are the energy terms from second and third order density fluctuations. 
The second order term, $E_\gamma$ is common to GFN1 and GFN2, and is given by

\begin{equation}
E_\gamma = \frac{1}{2} \sum^{N}_{A,B} \sum_{l \in A} \sum_{l' \in B} q_l q_{l'} \gamma_{AB, ll'},
\end{equation}
%
where $q_l$ are shell-resolved Mulliken partial charges. The $\gamma$ operator describes 
short-range Coulombic interactions

\begin{equation}
\gamma_{AB, ll'} = \frac{1}{\sqrt{R^2_{AB} + \eta^{-2}_{AB, ll'}}},
\end{equation}
%
where $R_{AB}$ is the internuclear distance between A and B, and $\eta$ is a parameterized 
chemical hardness. The third order term is slightly different for GFN1 and GFN2,
generally given as

\begin{equation}
E_\Gamma = \frac{1}{3}\sum_A^N q_A^3 \Gamma_A,
\end{equation}
%
where $q_A$ is the atom partial charge (sum of the shell partial charges on that atom), 
and $\Gamma_A$ is a different operator constructed from atom-wise parameters. These
terms are analogous to the $\gamma$ operators discussed in the general DFTB theory
section (equation \ref{eq:dftb_e_ex}). GFN2-xTB also includes higher order multipole 
interactions in the density-dependent terms, referred to as anisotropic electrostatic 
(AES) and anisotropic exchange-correlation (AXC) terms. The AES term is given by 
a sum of the monopole-dipole, monopole-quadrupole and dipole-dipole interactions

\begin{equation}
    E_{\text{AES}} = E_{q\mu} + E_{q \Theta} + E_{\mu\mu}
\end{equation}
%
The monopole-dipole interaction, monopole-quadrupole and dipole-dipole energy terms 
are given by

\begin{equation}
    E_{q\mu} = \frac{1}{2} \sum_{AB} f_3\left(R_{AB}\right) \left[ q_A \left(\mathbf{\mu}_B^T \mathbf{R}_{BA}\right) + q_B \left(\mathbf{\mu}_B^T \mathbf{R}_{AB}\right)\right]
\end{equation}
%
\begin{equation}
    E_{q \Theta} = \frac{1}{2} \sum_{AB} f_5\left(R_{AB}\right) \left[ q_A \mathbf{R}^T_{AB} \mathbf{\Theta}_B \mathbf{R}_{AB} + q_B \mathbf{R}^T_{AB} \mathbf{\Theta}_A \mathbf{R}_{AB} \right]
\end{equation}
%
\begin{equation}
    E_{\mu\mu} = \frac{1}{2} \sum_{AB} f_5\left(R_{AB}\right) \left( \mathbf{\mu}_A^T \mathbf{\mu}_B \right) R_{AB}^2 - 3 \left( \mathbf{\mu}_A^T \mathbf{R}_{AB} \right) \left( \mathbf{\mu}_B^T \mathbf{R}_{AB} \right) 
\end{equation}
%
respectively, where  $\mathbf{\mu}_A$ is the dipole moment on atom $A$ and where 
$\mathbf{\Theta}_A$ is the quadrupole moment. The damping functions $f_n \left(R_{AB}\right)$
follow a similar scheme to the dispersion models below, although with modified parameters.

The AXC term is given by 

\begin{equation}
    E_{\text{AXC}} = \sum_A \left( f_{XC}^{\mu_A} \left\lvert \mathbf{\mu}_A\right\rvert^2  + f_{XC}^{\Theta_A} \left\lvert \left\lvert \mathbf{\Theta}_A \right\rvert\right\rvert^2\right)
\end{equation}
%
where $f_{XC}^{\mu_A}, f_{XC}^{\Theta_A}$ are element specific parameters. It can
be seen that this exchange-correlation term only accounts for changes to the electron
density around atom $A$.

Whilst the energy terms discussed so far have been derived from the Taylor expansion
of electron density, other energy corrections are necessary to account for some 
of the shortcomings of the tight-binding approximations. The repulsion energy term
common to all methods describes nuclear-nuclear interactions,
different to the $E_{\text{rep}}$ term in DFTB. This is defined as

\begin{equation}
    E_{\text{rep}} = \frac{1}{2}\sum_{A,B} \frac{Z^{\text{eff}}_A Z^{\text{eff}}_B}{R_{AB}} e^{-\sqrt{\alpha_A \alpha_B} \left(R_{AB}\right)^{k_f}}
\end{equation}
%
where $Z^{\text{eff}}_A$ is the effective nuclear charge on atom $A$, differing 
from the true nuclear charge $Z_A$ by the core atomic electron density, $k_f$ is
a global parameter and $\alpha_A$ are atom-wise parameters.

The dispersion energy terms, whilst all correcting for the charge-average schemes
introduced by the Kohn-Sham approach, vary in models. The D3 dispersion model gives
this energy as

\begin{equation}
    E^{\text{D3}}_\text{disp} = -\frac{1}{2}\sum_{A,B}\sum_{n=6,8} s_n \frac{C_n\left(CN_A, CN_B\right)}{R^n_{AB}} f^{\left(n\right)}_{\text{BJ-damping}} \left(R_{AB}\right)
\end{equation}
%
where $C_n$ are the dispersion coefficients for dipole-dipole ($n=6$) and dipole-quadrupole
(n=8) interactions, which are functions of the coordination number $CN_A$, $s_n$
is a scaling factor, and the Becke-Johnson damping function \cite{Johnson2005, Becke2005} 
is given by

\begin{equation}
    f^{\left(n\right)}_{\text{BJ-damping}}\left(R_{AB}\right) = \frac{R^n_{AB}}{R^n_{AB} + \left(a_1 \sqrt{\frac{C_8^{AB}}{C_6^{AB}}} + a_2 \right)^n} 
\end{equation}
%
where $a_1$, $a_2$ are also global parameters. The D4 model of dispersion differs
by including a charge dependency in the dispersion coefficient function, giving

\begin{equation}
    E^{\text{D4}}_\text{disp} = -\frac{1}{2}\sum_{A,B}\sum_{n=6,8} s_n \frac{C_n\left(q_A, CN_A, q_B, CN_B\right)}{R^n_{AB}} f^{\left(n\right)}_{\text{BJ-damping}} \left(R_{AB}\right)
\end{equation}
%
(here the charges are calculated using the EEQ scheme, discussed below). For GFN2-xTB
the D4 model is modified to include a 3-body term, giving

\begin{equation}
    \begin{split}
    &E^{\text{D4}^{\prime}}_\text{disp} = -\frac{1}{2}\sum_{A,B}\sum_{n=6,8} s_n \frac{C_n\left(CN_A, CN_B\right)}{R^n_{AB}} f^{\left(n\right)}_{\text{BJ-damping}} \left(R_{AB}\right) \\
    & + s_9 \sum_{A,B,C} \frac{\left(3\cos\left(\theta_{ABC}\right)\cos\left(\theta_{BCA}\right)\cos\left(\theta_{CAB}\right)+1\right) C_9 \left(CN_A, CN_B, CN_C\right)}{\left(R_{AB}R_{AC}R_{BC}\right)^3} \\
    & \times f^{\left(n\right)}_{\text{damping}} \left(R_{AB}, R_{AC}, R_{BC}\right)
    \end{split}
\end{equation}
%
where $f^{\left(n\right)}_{\text{damping}} \left(R_{AB}, R_{AC}, R_{BC}\right)$ is
a special damping function for the three-body term. It can be seen that again GFN2-xTB 
includes higher order terms to achieve greater accuracy against the test data.

Also present in GFN1- and GFN0-, but not GFN2-xTB, are correctional terms that are
independent with respect to the electron density. For GFN1-xTB this is the halogen
bonding energy term, which for a system of halogen bond acceptor $A$, donor $B$ and
halogen $X$ is given by

\begin{equation}
    E_{XB}^{\text{GFN}1} \sum^{N_{XB}}_{AXB} f_{ABX\text{-damping}} k_X \left[ \left( \frac{k_{XR} R_{\text{cov}, AX}}{R_{AX}}\right)^{12} - k_{X2} \left(\frac{k_{XR} R_{\text{cov}, AX}}{R_{AX}}\right)^6 \right] \left[ \left( \frac{k_{XR} R_{\text{cov}, AX}}{R_{AX}}\right)^{12} + 1\right]^{-1}
\end{equation}
%
where $k_{X2}, k_{XR}$ are global parameters but $k_X$ is a halogen-specific parameter.
The damping term is given as

\begin{equation}
    f_{AXB\text{-damping}} = \frac{1}{2} \left(1 - \frac{1}{2} \frac{R^2_{XA} + R^2_{XB} - R^2_{AB}}{\left\lvert R_{XA} \right\rvert \left\lvert R_{XB} \right\rvert} \right) ^6
\end{equation}
%
Similar to DFT and DFTB, these energy terms (or more accurately their potentials)
are used to construct the Fock matrix that is solved self-consistently in the Roothan-Hall
equations to give ground state MO coefficients and energies. However this is only
true for GFN1- and GFN2-xTB, as these use Mulliken schemes for the partial charges.
The GFN0-xTB uses a non-self consistent charge scheme referred to as electronegativity
equilibration (EEQ), which means that only one diagonalization of the Fock matrix
is required for a ground state solution. These charges are given by solving the 
matrix equation

\begin{equation}
    \label{eq:eeq_solve}
    \begin{pmatrix}
        \mathbb{A} & \mathbf{1} \\
        \mathbf{1}^T & 0 \\
    \end{pmatrix}
    \begin{pmatrix}
        \mathbf{q} \\
        \lambda \\
    \end{pmatrix}
    \begin{pmatrix}
        \mathbf{X} \\
        q_\text{tot} \\
    \end{pmatrix}.
\end{equation}
%
In this scheme, $q_\text{tot}$ is the total charge of the system, $\mathbf{X}$ is
a vector of electronegativities given by

\begin{equation}
    X_A = \kappa_A \sqrt{mCN_A} - EN_A
\end{equation}
%
where $mCN_A$ is a modified coordination number, and $\mathbb{A}$ is a charge-charge
interaction matrix that damps interatomic interactions and returns a measure of 
the chemical hardness for intra-atomic elements. The diagonal (single atom) and 
off-diagonal (atom pair) matrix elements are given by

\begin{equation}
    \mathbb{A}_{AA} = J_{AA} + \frac{2\gamma_{AA}}{\sqrt{\pi}}
\end{equation}
%
\begin{equation}
    \mathbb{A}_{AB} = \frac{\text{erf}\left(\sigma_{AB} R_{AB}\right)}{R_{AB}},
\end{equation}
%
where $\sigma_{AB}$ is a geometric mean of the atomic radii $\alpha_A$. Solving 
for these charges then gives the energy as 

\begin{equation}
    E_{EEQ} = \mathbf{q}^T \left(\frac{1}{2}\mathbb{A}\mathbf{q} - \mathbf{X}\right)
\end{equation}
%
These charges are used in place of Mulliken charges that would be found in other
energy terms, and as only a single diagonalization of equation \ref{eq:eeq_solve} 
is required to generate these charges it can be seen that GFN0-xTB does not need
a self-consistent approach to get ground state solutions.

For the most part, the GFN-xTB methods use a minimal basis set, however with some
exceptions for hydrogen atoms. These are constructed from Gaussian functions linearly
combined into Slater type orbitals. The coefficients and number of Gaussian functions
are intrinsic to each GFN-xTB method.

The parameters for the energy terms are either taken from DFT properties (such
as the electronegativities or covalent radii) or by using a "top-down" approach
to optimize the parameters to a set of target properties. This approach is very 
successful for all methods. GFN1-xTB  has a standard relative deviation of around
1.1\% when compared to geometries from higher level methods, with GFN2-xTB performing
similarly well. Generally the GFN-xTB methods predict non-covalent interaction energies
with a mean absolute deviation of just over 1 kcal/mol, comparable to low level 
DFT methods and greatly outperforming other semi-empirical quantum methods. These
successes against the target data shows that using partial charge interactions to
replace full integrals and global/element-wise parameters to scale interactions 
works well in designing efficient and accurate methods.

\section{Excited State Methods}
\label{sec:response_theories}

\subsection{Linear Response TD-DFT}
\label{subsec:tddft}

Linear response time-dependent DFT (TD-DFT) is a well established method for calculating
excitation energies and transition properties from only ground state information \cite{Laurent2013}. 
It is formulated from the Runge-Gross theorem \cite{Runge1984}, which states that the
time dependent density of a system can be mapped from the time-dependent external 
potential (for light-matter interactions, the external potential is the light wave), 
and this mapping is unique. This is analogous to the Hohenberg-Kohn theorems used
to derive ground states in section \ref{subsec:dft}. Similar to the Taylor expansion
approach used to derive energy terms for DFTB theory, the density response is usually
expanded in terms of time derivatives. This is valid as long as the time scale over
which the systems responds to the external perturbation short, and the excited state
density is not too different from the ground state. The name "linear response" is
due to curtailing this expansion at first orders terms, which makes TD-DFT practical 
to calculate \cite{Marques2004}. This is useful as only the ground state is needed 
to calculate perturbations to the first order, meaning that all transition properties
can be calculated from the ground state \cite{Marques2004}.

Response theory is used to predict the changes in the electronic structure of a 
system over a time period, defined by the time-dependent Schr\"{o}dinger equation

\begin{equation}
    i \frac{\delta}{\delta t} \Psi\left(r, t\right) = \hat{H}\left(r, t\right) \Psi\left(r, t\right)
\end{equation}
%
For excited states, this change is in response to an external light potential, defining
the Hamiltonian as

\begin{equation}
\hat{H}\left(t\right) = \hat{H^0} + v_{\text{ext}}\left(t\right)
\end{equation}
%
where $\hat{H^0}$ is the unperturbed Hamiltonian, $v^{\text{ext}}\left(t\right)$ 
is the potential from the external field, which is usually taken to be an oscillating 
electric field (such as that created by a photon) and so is time $t$ dependent. 
This Hamiltonian can then be used to describe a time-dependent set of Kohn-Sham 
equations \cite{Kohn1964}. The time-dependent Kohn-Sham Hamiltonian is given by

\begin{equation}
\hat{H}_{KS}\left[\rho\left(t\right)\right] = \hat{H}^0_{KS}\left[\rho\right] + v_H\left[\rho\left(t\right)\right] + v_{\text{XC}}\left[\rho\left(t\right)\right] + v_{\text{ext}}\left(t\right)
\end{equation} 
%
where $v_H$ and $v_{XC}$ are the Kohn-Sham Coulomb and exchange-correlation potentials 
respectively. The Runge-Gross theorem states that the density solution to this Hamiltonian
can be uniquely mapped from the potential function, and so the time-dependent density 
can be written as a functional of the potential function

\begin{equation}
\rho\left(t\right) = \rho\left[v_{\text{ext}}\left(t\right)\right].
\end{equation}
%
The response of the density can be given by the integral of this external potential
with what is referred to as the response function $\chi$

\begin{equation}
    \delta \rho \left(r, \omega \right) = \int d^3 r^\prime \chi \left(r, r^\prime, \omega \right) \delta v_{\text{ext}} \left(r^\prime, \omega \right),
\end{equation}
%
which, when calculated from the Fourier transform of the time series (i.e. calculated 
in the frequency space) can be written explicitly for a Kohn-Sham system of electrons 
as

\begin{equation}
    \chi_{KS} \left(r, r^\prime, \omega \right) = \sum_{\mu\nu} \left(f_\mu - f_\nu \right) \frac{\psi_\mu\left(r\right)\psi^\ast_\mu\left(r^\prime\right) \psi_\nu\left(r^\prime\right) \psi^\ast_\nu\left(r\right)}{\omega - \left(\epsilon_\mu - \epsilon_\nu\right) + i\eta}
\end{equation}
%
where $f_\mu$ is the occupation number of the orbital $\psi_\mu$, and $\eta$ is a positive
infinitesimal number. Again similar to ground-state DFT, this response function 
is used in conjunction with correcting exchange-correlation terms to yield the response
function of the real system. This approach would calculate the response of the electron
density, and therefore the  excited state, explicitly. However the issue with the
theory given so far is that it does not identify which frequencies $\omega$ give
any appreciable density response i.e. where the excited states are. Gross \emph{et al.}
solved this issue by writing the (non-KS) response function as in a Lehmann representation,
with a discretized set of states $m$

\begin{equation}
    \chi\left(r, r^\prime, \omega \right) = \lim_{\eta \to 0^+} \sum_m \left[ \frac{\braket{0|\hat{n}\left(r\right)|m}\braket{m|\hat{n}\left(r^\prime\right)|0}}{\omega - \left(E_m - E_0\right) + i\eta} - \frac{\braket{0|\hat{n}\left(r^\prime\right)|m}\braket{m|\hat{n}\left(r\right)|0}}{\omega + \left(E_m - E_0\right) + i\eta} \right]
\end{equation}
%
where $\ket{0}$ is the ground state with energy $E_0$, $\ket{m}$ is an excited state
with energy $E_m$, and $\hat{n}$ is the density operator \cite{Petersilka1996}. 
It can be seen that the poles of the response function (the places where it equals zero)
would be where $\omega = E_m - E_0$ or where the frequencies of light match the 
excitation energies. Eventually it can be shown that these poles can be identified
by solving the eigenvalue equation

\begin{equation}
\label{eq:full_cassida_eq1}
\left(\begin{matrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{B^*} & \mathbf{A^*}
\end{matrix}\right)
\left(\begin{matrix}
\mathbf{X}\\
\mathbf{Y}
\end{matrix}\right)
=
\left(\begin{matrix}
\omega & 0\\
0 & -\omega
\end{matrix}\right)
\left(\begin{matrix}
\mathbf{X}\\
\mathbf{Y}
\end{matrix}\right)
\end{equation}
%
where $\omega$ are the excitation energies, and the vectors $\mathbf{X}$ and $\mathbf{Y}$ 
describe the electronic transitions in the basis of ground state molecular orbitals.
This equation is often referred to as the Casida equation \cite{Casida1995}, with 
a full derivation also given in later reviews \cite{Casida2004, Marques2012}. The
elements of matrices $\mathbf{A}$ and $\mathbf{B}$ are given by

\begin{equation}
A_{ia,jb}\left(\omega\right) = \delta_{ij}\delta_{ab}\left(\epsilon_a - \epsilon_i\right) + \int dr \int dr^\prime \psi_i^*\left(r\right) \psi_a\left(r\right) f_{\text{XC}}\left(r, r^\prime, \omega\right) \psi_i\left(r^\prime\right) \psi_a^*\left(r^\prime\right)
\label{eq:full_A_integrals}
\end{equation}
%
\begin{equation}
B_{ia,jb}\left(\omega\right) = \int dr \int dr^\prime \psi_i^*\left(r\right) \psi_a\left(r\right) f_{\text{XC}}\left(r, r^\prime, \omega\right) \psi_i\left(r^\prime\right) \psi_a^*\left(r^\prime\right)
\end{equation}
%
where $i$,$j$ and $a$, $b$ are occupied and virtual orbital indices respectively, $\epsilon_i$
are the energies of the ground state orbitals, and $\delta$ is the usual kronecker 
delta function. The $\mathbf{A}$ matrix include occupied-to-virtual transition contributions,
whilst the $\mathbf{B}$ matrix includes virtual-to-occupied transitions - these 
are also loosely referred to as excitation and "de-excitation" contributions \cite{Casida2012}.  
The kernel function $f_{\text{XC}}$ is an exact frequency (excitation energy) 
dependent exchange-correlation functional and as it is dependent on the excitation 
energy given by the solutions of this eigenvalue equation, can be seen to be self-consistent.
To make these functionals computable this self-consistency is neglected, such that
the $\mathbf{A}$ and $\mathbf{B}$ matrix elements can be calculated from ground
state properties. These could be provided by both DFT and DFTB methods.

Using TD-DFT for LHC investigations come with some caveats. Fairly early on it was 
found that the charge-transfer artifacts can affect low energy ($Q$ band) excitations
\cite{Dahlbom2005}. There is also difficulty in obtaining good reference data - 
for example coupled-cluster methods are extremely expensive for chlorophyll systems
as they contain $\sim 140$ atoms. A recent successful benchmarking of density functionals 
compares TD-DFT to DFT/MRCI - DFT/MRCI was chosen in this example as a reference 
method due to its good performance against \emph{ab initio} methods for $\pi$ conjugated
systems including chlorophyll \cite{Marian2008, Perun2008, Etinski2011, Parusel2000}. 
In the TD-DFT benchmarking, a range of different density functionals were used to
calculate the \Qy transitions in chlorophylls from the FMO complex, and it was found
that PBE0, CAM-B3LYP and B3LYP had RMSEs in transition energies of less than 0.05 
eV against DFT/MRCI \cite{List2013}. CAM-B3LYP also proved the best in reproducing 
the whole $Q$ band ($Q_x$ and \Qy). With few alternatives, TD-DFT is a common choice
for calculating chlorophyll excited states.

\subsection{Eigenvalue Difference}
\label{subsec:eigval_diff}
One approximation to full response theory is the eigenvalue difference method. For
eigenvalue difference method, it is assumed there is no response of the orbital 
energies and shapes when interacting with light. This approach can be recovered 
from the complete Cassida equation (equation \ref{eq:full_cassida_eq1}) if the coupling
elements (off-diagonal elements) in the $\mathbf{A}$ and $\mathbf{B}$ matrices are
set to zero. Within this approximation, the transition energy is just the difference
between the ground state energy of the orbital an electron has been excited to ($\epsilon_{\text{2}}$)
and the orbital has been excited from ($\epsilon_{\text{1}}$)

\begin{equation}
\Delta E = \epsilon_{\text{2}} - \epsilon_{\text{1}}.
\end{equation}
%
Generally, eigenvalue difference methods are not seen as accurate response methods,
but can offer a quick and easy initial value \cite{Gimon2009}.

\subsection{$\Delta$-SCF}
\label{subsec{dscf_and_eigdiff}}

\dscf \cite{Jones1989, Hellman2004} predicts the transition energy $\Delta E$ of 
a system as the difference of the single point energy $E_n$ of two states:

\begin{equation}
\Delta E = E_{2} - E_{1}.
\end{equation}
%
Assuming that the excited state solution will be in a similar location to the ground
state in the MO coefficient space, the ground-state MO coefficients can be used 
as an initial guess for the excited state. In its simplest form, the \dscf method 
calculates the ground-state with normal DFT or other mean-field methods and then
calculates the excited state by rerunning the same method with the excited state
occupation numbers. The second set of MO coefficients then give a full description
of the excited state.

The issue with finding the excited state solution is that the variation principle
and SCF iterative procedure will try to find the global minimum, which is the ground
state. The excited state is only a local minimum, and so often is less reliable
to find as a solution especially from the standard SAD initial guess \cite{Almlof1982} 
(the SAD or "superposition of atomic orbitals" is a standard initial guess of the
ground state density, calculated as a combination of electronic densities calculated
for each atom of the system in isolation.) For this reason it is often found that 
converging to the \dscf excited state will fail. Even when using the ground state 
as an initial guess with excited state occupations, normal SCF procedure may still 
collapse back to the ground state. Usually it is necessary to include additional 
changes to the SCF procedure, such as Fock damping (mixing of previous iterations'
Fock matrices with the current Fock matrix to reduce or dampen "jumps" which may
prevent convergence), DIIS methods (quadratic extrapolation of Fock matrices) and
sometimes intermediate initial guess steps (i.e. using excited state populations 
with fractional occupations before integral occupations).

In early version of \dscf, the excited state was calculated by relaxing the orbitals
which contain the excited electron and hole in the ground state space, so that the
excited state and ground state are orthogonal \cite{Hunt1969}. However, it was
argued that this procedure would exacerbate the likelihood of collapsing to the ground
state, and that the excited state was not a proper SCF solution \cite{Gilbert2008}.
Consequently, an alternative SCF-like method was proposed, where instead of populating orbitals 
according to the Aufbau principle, orbitals which most resemble the previous iteration's
orbitals should be occupied. This is known as the maximum overlap method (MOM) \cite{Gilbert2008}. 
The maximum overlap method starts with the normal SCF procedure, where each iteration
produces new molecular orbital coefficients by solving the Roothaan-Hall equations, \cite{Roothaan1951}

\begin{equation}
\mathbf{F} \mathbf{C}^{\text{n}} = \mathbf{S} \mathbf{C}^{\text{n}} \epsilon,
\label{eq:roothaan_hall}
\end{equation}
%
where $\mathbf{C}^{\text{n}}$ are the $n^{\text{th}}$ orbital coefficient solutions, 
$\mathbf{S}$ is the overlap of orbitals, and $\epsilon$ are the orbital energies. 
The Fock matrix $\mathbf{F}$ is calculated from the previous set of orbital coefficients,

\begin{equation}
\mathbf{F} = f\left(\mathbf{C}^{n-1}\right).
\end{equation}
%
At this point MOM differs to normal SCF by populating set of orbitals with the most
similarity to the previous orbitals The amount of similarity in orbitals can be
estimated from their overlap

\begin{equation}
\mathbf{O} = \left(\mathbf{C}^{\text{old}}\right)^\dagger \mathbf{S} \mathbf{C}^{\text{new}},
\end{equation}
%
which for a single orbital can be evaluated as a projection

\begin{equation}
p_j = \sum^n_i O_{ij} = \sum^N_\nu \left[\sum^N_\mu\left(\sum^n_i C_{i\mu}^{\text{old}}\right)S_{\mu\nu}\right]C^{\text{new}}_{\nu j}
\end{equation}
%
where $\mu,\nu$ are orbital indices. This method can be used for any excited state,
with the caveat that the orbital solution will most likely be in the same region
in the solution space as the ground state solution. For a small number of low-lying
states this is generally true, and so \dscf can be used to calculate a small spectrum 
of excited states \cite{Gilbert2008}. A similar method referred to as iMOM or initial-MOM
determines population by similarities to the initial orbital coefficients, which 
can be beneficial for some convergence issues \cite{Barca2018}.

\dscf has been shown to be cheap alternative to TD-DFT and other higher level methods 
\cite{Liu2004, Gavnholt2008, Besley2009} without considerable losses of accuracy 
in certain cases especially for HOMO-LUMO transitions. Kowalczyk \emph{et al.} showed 
that for a test set of small to medium $\pi$ conjugated systems, including a unsubstitued
porphyrin ring, \dscf can reproduce experimental HOMO-LUMO transitions energies 
about as well as PBE0 TD-DFT \cite{Kowalczyk2011}. Additionally, as the excited 
state is given as solutions to SCF equations, the gradient of this solution with 
respect to atomic positions can be given similar to normal mean-field theory. These
gradients would be much cheaper than TD-DFT or coupled cluster methods, which is
advantageous for simulating dynamics \cite{Gavnholt2008} as well as a vibrationally 
resolved absorption spectra \cite{Kowalczyk2011}.

\subsubsection{Transition Density and Dipole Moments}
\label{subsec:dscf_transition_density}
Transition density matrices are a well used tool in excited state analysis \cite{McWeeny1960, Tretiak2002}. 
Detailed interpretations can sometimes differ (e.g. for excited states from CIS/TDA 
and normal TD-DFT \cite{Etienne2015}) but in general transition density matrices
describe the change in electronic structure in going from one state to another.
Key to this discussion is the fact that observables, such as the transition dipole, 
can be calculated by tracing operators with the transition density matrix. For \dscf 
the transition density is calculated from the SCF solutions for the ground and excited
states. The reduced one-particle transition density matrix $\mathbf{D}^{21}$ can
be written as

\begin{equation}
\mathbf{D}^{21} = \ket{\Psi_1} \bra{\Psi_2},
\end{equation}
%
where $\ket{\Psi_n}$ is the Slater determinant of state $n$, constructed from the
set of spin orbitals $\{ \phi_{j}^{\left(n\right)} \} $. Expressed in terms of the
molecular orbital coefficients $\mathbf{C}^{\left(n\right)}$, the transition density
matrix is

\begin{equation}
\mathbf{D}^{21} = \mathbf{C}^{\left(2\right)} \text{adj}\left(\mathbf{S}^{21}\right) \mathbf{C}^{\left(1\right) \dagger},
\end{equation}
%
where $\mathbf{S}^{21}$ is an overlap matrix with elements 

\begin{equation}
S^{21}_{jk} = \braket{\phi^2_j|\phi^1_k}.
\label{eq:lowdin_overlap}
\end{equation}
%
Equation \ref{eq:lowdin_overlap} is derived using L{\"o}wdin's rules for non-orthogonal
determinants \cite{Lowdin1955}. Observable properties of the excited state can be
calculated by tracing appropriate operator with the transition density matrix. For
example, the transition dipole moment is found by taking the trace of the transition
density matrix with the dipole operator

\begin{equation}
    \begin{aligned}
        \mu^{1\rightarrow2} &= \text{Tr}\left(\hat{\mu} \mathbf{D}\right)
        &= \text{Tr}\left(\hat{\mu} \ket{\psi_1} \bra{\psi_2}\right)
        &= \braket{\Psi_2|\hat{\mathbf{\mu}}|\Psi_1} 
        &= \sum_{jk} \mathbf{\mu}_{jk}^{21} \text{adj} \left( \mathbf{S}^{21}\right)_{jk}
    \end{aligned}
\end{equation}
%
where $\hat{\mathbf{\mu}}$ is the one-electron transition dipole operator, and
$\mu_{jk}$ is the element of this operator corresponding to orbital indices $j$, $k$.
The determinant of $\mathbf{S^{21}}$ is equal to the inner product of the two states
involved in the transition

\begin{equation}
\left\lvert {\mathbf{S}^{21}} \right\rvert = \braket{\Psi_2|\Psi_1}.
\end{equation}
%

\section{Excitation Energy Transfer and Frenkel-Davydov Exciton Hamiltonians}
\label{sec:frenkel_exciton_theory}

Excitation energy transfer (EET) and subsequent rates of transfer describe the time-frame
that energy, absorbed through a photon of light for LHCs, is shuttled between states.
It is thought that light harvesting systems have evolved to fine these rates \cite{Cleary2013}.
In regimes with strong coupling (ie large transfer), excitation energy can flow back
and forth between different sites. The Frenkel-Davydov model \cite{Frenkel1931, Davydov1964}
is used in the regime where the excited electron and its "hole" have a strong interaction
and so are localized on the same chromophore - this means mainly vertical excitations
are included instead of charge transfer states (although charge transfer states
are important mechanisms for the fluorescence quenching of chlorophyll excited states,
as discussed in section \ref{sec:concentration_quenching}).

For a dimer system with chromophores $A$ and $B$, there are two possible states 
where one molecule is in the ground state and the other in the excited state - these 
are labelled $\ket{A^*B}, \ket{AB^*}$ where $A^*$ denotes molecule $A$ in the excited
state. The exciton state is defined by a linear combination of these two states

\begin{equation}
\ket{\phi} = c_1 \ket{A^*B} + c_2 \ket{AB^*},
\end{equation}
%
where $c_n$ are the coefficients of each possible state. In systems beyond dimers 
other chromophore sites can be easily included in the basis states (i.e. $\ket{ABC^*}, 
\ket{ABCD^*}$ etc.). Calculating the coefficients $c_n$ requires diagonalising the
exciton Hamiltonian which can be written as a sum of individual site Hamiltonians 
and inter-site interactions

\begin{equation}
\hat{H}_{\text{tot}} = \sum^N_m \hat{H}_m + \sum^N_m \sum^N_n \left(\hat{V}_{\text{el-el}} + \hat{V}_{\text{el-nuc}} + \hat{V}_{\text{nuc-nuc}}\right),
\end{equation}
%
where $N$ is the number of sites, $\hat{H}_m$ is the electronic Hamiltonian for 
a single chromophore molecular m and $\hat{V}_{\text{el-el}}$, $\hat{V}_{\text{el-nuc}}$ 
and $\hat{V}_{\text{nuc-nuc}}$ are the electron-electron, electron-nuclear and nuclear-nuclear
interactions between two sites respectively. It is assumed that $H_m$ satisfies
the time-independent Schr√∂dinger equation

\begin{equation}
\hat{H}_{a_m} \ket{\phi_{a_m}} = E_{a_m} \ket{\phi_{a_m}}
\end{equation}
%
where ${a_m}$ indexes the electronic states on chromophore $m$. The total Hamiltonian
can be represented in a basis of eigenfunctions $\phi_{a_m}$ of the single chromophore 
Hamiltonians as

\begin{equation}
\bra{\Phi_a} \hat{H}_{\text{tot}}\ket{\Phi_b} = \sum^N_m \bra{\Phi_a} \hat{H}_{m}\ket{\Phi_b} + \sum^N_m \sum^N_n \bra{\Phi_a} \hat{V}_{\text{el-el}}\ket{\Phi_b}.
\label{eq:full_frenkel_hamil}
\end{equation}
%
This Hamiltonian neglects nuclear-electron and nuclear-nuclear interactions in favour
of treating the surrounding nuclear environment in the individual site Hamiltonians 
\cite{Scholes2003} - this includes the other chromophore sites as well as any other 
environments ie. the LH2 protein environment. Alternatively, this term can just be ignored.

It is also common to assume that only one excited state on each chromophore participates
in the energy transfer, and this is the first excited state. This reduces the size 
of the basis set, and is known as the Heitler-London approximation \cite{Agranovich2000}.
Additionally it is assumed that the chromophore sites are well separated enough 
that the wavefunctions do not overlap \cite{Frenkel1931}. Consequently equation 
\ref{eq:full_frenkel_hamil} reduces to

\begin{equation}
\bra{\Phi_a} \hat{H}_{\text{tot}}\ket{\Phi_b} = E^{\left(m\right)}_{a_m} \prod^N_l \delta_{al} \delta_{bl} + \sum^N_m \sum^N_n \bra{\Phi_a} \hat{V}_{\text{el-el}}\ket{\Phi_b}.
\label{eq:reduced_frenkel_hamil}
\end{equation}
%
The second term needs to be expanded into the basis set of individual chromophore 
sites so we can calculate this term from individual response calculations. Each 
term in the double summation can be given as

\begin{equation}
\begin{aligned}
\bra{\Phi_a} \hat{V}_{\text{el-el}}\ket{\Phi_b} &= \sum_{i \in \mathbf{r}_m} \sum_{j \in \mathbf{r}_n} \bra{\Phi_a} \frac{1}{\left|\mathbf{r_i} - \mathbf{r_j}\right|} \ket{\Phi_b} \\
&= \left(\sum_{i \in m} \sum_{j \in n} \bra{\phi^{\left(m\right)}_{a_m} \phi^{\left(n\right)}_{a_n}} \frac{1}{\left|\mathbf{r_i} - \mathbf{r_j}\right|} \ket{\phi^{\left(m\right)}_{b_m}\phi^{\left(n\right)}_{b_n}}\right)\prod^N_{l\neq m,n}\delta_{a_l, b_l}
\end{aligned}
\end{equation}
%
where $i,j$ are the indices of electrons on the sites $m,n$, and $\mathbf{r}_i, 
\mathbf{r}_j$ are their positions. With a few extra steps not included for brevity,
this expression can then be written in terms of the transition densities of each 
site\cite{Scholes2003}

\begin{equation}
\label{eq:coupling_density}
\bra{\Phi_a} \hat{V}_{\text{el-el}}\ket{\Phi_b} = \int d\mathbf{r}_{m} \int d\mathbf{r}_{n} \frac{\rho_{a_m}\left(\mathbf{r}_{m}\right)\rho_{b_n}\left(\mathbf{r}_{n}\right)}{\left|\mathbf{r}_{m} - \mathbf{r}_{n}\right|}
\end{equation}
%
where $\rho_{a_m}$ is the transition density of transition $a$ on site $m$. Often
the Frenkel Exciton Hamiltonian is summarised as

\begin{equation}
\hat{H}_{\text{eff}} = \sum^N_{m=1} \epsilon_m + \sum^{N,N}_{m \neq n} V_{mn}
\end{equation}
%
where the diagonal elements $\epsilon_m$ are site energies of the chromophores, 
and the off-diagonal elements $J_{mn}$ are the Coulombic coupling between the transition
density of different sites. It is usual to reduce the transition density with a 
multipole expansion \cite{Steinmann2015}. This could either by done on a whole site
scale or at a more detailed atom-in-site scale. Looking at the coarser-grained whole
site scale first, the first term in the reduction, corresponding to monopoles, is
zero, as a local excitation will not produce an overall transition charge (as opposed 
to a non-local charge-transfer excitation, which can be included in the Frenkel 
Hamiltonian in some cases \cite{Li2017}). Using the second term in this expansion 
gives a dipole-dipole interaction, referred to as the point-dipole method

\begin{equation}
    V_{mn} = \frac{\mu_m \mu_n}{R_{mn}^3} - 3 \frac{\left(\mu_m \cdot \mathbf{R}_{mn}\right) \left( \mu_n \cdot \mathbf{R}_{mn} \right)}{R_{mn}^5}
\end{equation}
%
where $\mu_m$ is the transition dipole for site $m$ and $R_{mn}$ is the distance
between sites $m$ and $n$. Alternatively the transition density can also be reduced 
to atom centred charges. This gives the coupling as a Coulombic interaction

\begin{equation}
    V_{mn} = \sum_{A>B,A \in m, B \in n}\frac{q_A q_B}{\left\lvert \mathbf{R}_A - \mathbf{R}_B \right\rvert}
\end{equation}
%
where $A$, $B$ are indices for atoms on sites $m$ and $n$ respectively, $\mathbf{R}_A$, $\mathbf{R}_B$
are the position vectors for these atoms and $q_A$, $q_B$ are the atom centered 
transition charges.

Other methods for calculating coupling interactions include as the extended dipole \cite{Madjet2006a},
transition charges from electrostatic potential \cite{Madjet2006a, Renger2009} and
transition density cube methods (i.e. a grid approach of discretizing the continuous
transition density) \cite{Krueger1998}. Additionally calculating the coupling value
straight from equation \ref{eq:coupling_density} is possible and necessary for benchmarking, 
but is the most expensive option \cite{Kenny2016}. Intuitively using a more detailed
description of the site transition density results in more accurate couplings, however
past the point dipole method (which often overestimates coupling energies) there
are diminishing returns in using more expensive methods. At large separations (>$20 \AA{}$), 
all of these methods converge to the same value \cite{Kenny2016}.

Regardless of the interaction method used, all that is needed to calculate a rough
Frenkel exciton Hamiltonian is the local excitations and transition properties for 
each site.
 
